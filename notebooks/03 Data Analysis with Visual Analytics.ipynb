{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis with Visual Analytics\n",
    "**Combining some basic analytics with visualization**\n",
    "\n",
    "NOTE: goal is to show benifit of easy RAPIDS framework compatability, and speed of which can do iterations on large(ish) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Requirements\n",
    "Super short version of intro notebook and restate requirments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "import cupy\n",
    "import cuspatial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "\n",
    "import hvplot.cudf\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Data / Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_csv(DATA_DIR / \"data.csv\", parse_dates=('starttime', 'stoptime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv(DATA_DIR / \"stations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some analysis -> ? = profit?\n",
    "Mix of cuML / cuspatial analysis to then visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to cuGraph / cuSpatial / cupy\n",
    "\n",
    "We will analyze the data using some RAPIDS tools:\n",
    "\n",
    "* [cuGraph](https://docs.rapids.ai/api/cugraph/stable/) is a collection of GPU accelerated graph algorithms that process data found in GPU DataFrames. We can use to compute pagerank or degree measure on the trip graph. \n",
    "\n",
    "* [cuSpatial](https://docs.rapids.ai/api/cuspatial/nightly/) is a collection of GPU accelerated algorithms for computing geo-spatial measures. We can use to to compute trips with given bounding regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial\n",
    "\n",
    "Let's take a look at some spatial measures and see if there are any interesting features.\n",
    "\n",
    "We might start with the first station, and see what the min/max trip length from it are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0 = df.iloc[0]\n",
    "origin_lon, origin_lat = r0[\"longitude_start\"], r0[\"latitude_start\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cuSpatial function `lonlat_to_cartesian` will let us quicly compute the x/y distances for every ending trip location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = cuspatial.lonlat_to_cartesian(origin_lon[0], origin_lat[0], df[\"longitude_end\"], df[\"latitude_end\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CuPy functions can compute derived values on these GPU dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy.sqrt(cupy.max(dist.x**2 + dist.y**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to compute this max/min trip interval for every station as a starting point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_intervals = []\n",
    "\n",
    "for idx, row in stations.iterrows():\n",
    "    station_id, origin_lon, origin_lat = int(row[\"station_id\"]), row[\"lon\"], row[\"lat\"]\n",
    "    dist = cuspatial.lonlat_to_cartesian(origin_lon, origin_lat, df[\"longitude_end\"], df[\"latitude_end\"])\n",
    "    from_intervals.append((station_id, float(cupy.sqrt(cupy.max(dist.x**2 + dist.y**2)))))\n",
    "\n",
    "from_intervals = pd.DataFrame(from_intervals, columns=[\"station_id\", \"dist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_longest = from_intervals.nlargest(25, \"dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_longest_loc = stations[stations.station_id.isin(from_longest.station_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_longest_loc.hvplot.points(x='lon', y='lat', size=300, geo=True, tiles=\"OSM\").opts(width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagerank\n",
    "\n",
    "We will use the `cugraph.pagerank` function to see if there are patterns for the \"most popular\" stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"weekday\"] = df['starttime'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single hour \n",
    "\n",
    "Let's see what it looks like to compute page range for a single hour of the day, e.g. 5PM. First subset the data to only look at data for trips starting at that hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17 = df[df[\"hour\"]==17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next group by (from_station_id, to_station_id) and then take the group size to get all the unique indivual routes between stations that hour, and also the number of trips that took each of those routes:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g17 = df.groupby(by=[\"from_station_id\", \"to_station_id\"])\n",
    "routes17 = g17.size().reset_index()\n",
    "routes17.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a `cugraph.Graph` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cugraph.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.from_cudf_edgelist(d17, source='from_station_id', destination='to_station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17_page = cugraph.pagerank(G)\n",
    "d17_page.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17_top = d17_page.nlargest(10, \"pagerank\").to_pandas()\n",
    "d17_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17_page_locs = stations[stations.station_id.isin(d17_top.vertex)]\n",
    "d17_page_locs.hvplot.points(x='lon', y='lat', size=300, geo=True, tiles=\"OSM\").opts(width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the same  process and see the highest ranked stations at noon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d8 = df[df[\"hour\"]==12]\n",
    "g8 = df.groupby(by=[\"from_station_id\", \"to_station_id\"])\n",
    "routes17 = g8.size().reset_index()\n",
    "\n",
    "G = cugraph.Graph()\n",
    "G.from_cudf_edgelist(d8, source='from_station_id', destination='to_station_id')\n",
    "d8_page = cugraph.pagerank(G)\n",
    "d8_page.head()\n",
    "\n",
    "d8_top = d8_page.nlargest(10, \"pagerank\").to_pandas()\n",
    "\n",
    "d8_page_locs = stations[stations.station_id.isin(d8_top.vertex)]\n",
    "d8_page_locs.hvplot.points(x='lon', y='lat', size=300, geo=True, tiles=\"OSM\").opts(width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how stations rank week on weekdays vs weekends. The code below computes the pagerank broken out by individual day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for w in range(7):\n",
    "    dfw = df[df[\"weekday\"]==w]\n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(dfw, source='from_station_id', destination='to_station_id')\n",
    "    df_page = cugraph.pagerank(G).nlargest(20, \"pagerank\")\n",
    "    results[w] = set(df_page.to_pandas()[\"vertex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find out what stations were highest ranked among all weekdays and weekend days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = set.intersection(*[results[i] for i in range(5)])\n",
    "weekend = set.intersection(results[5], results[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can plot these quickly using `hvplot`. Let's add a column to denote weekday/weekend so that we can group by that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = stations[stations.station_id.isin(weekend)]\n",
    "r1 = r1.assign(type=\"Weekend\")\n",
    "\n",
    "r2 = stations[stations.station_id.isin(weekday)]\n",
    "r2 = r2.assign(type=\"Weekday\")\n",
    "\n",
    "result = pd.concat([r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.hvplot.points(x='lon', y='lat', by='type', alpha=0.5, size=485, geo=True, tiles=\"OSM\").opts(width=800, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of interesting analytics results \n",
    "Does not have to be significant but noteable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to group by day, we first take the \"floor\" of each timestamp divided by one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = np.datetime64(1, 'D').astype('datetime64[ns]').astype('int64') \n",
    "\n",
    "df['from_day'] = df['starttime'].astype('int64') // one_day\n",
    "df['to_day'] = df['stoptime'].astype('int64') // one_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can group by the station id and hour for both the departing and arriving cases. We name the columns from the size DataFrame `out` and `in` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df.groupby(by=[\"from_station_id\", \"from_day\"]).size().to_frame('out').reset_index()\n",
    "df_in = df.groupby(by=[\"to_station_id\", \"to_day\"]).size().to_frame('in').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re name the columns to be the same in both DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.rename(columns={\"from_station_id\": \"station_id\", \"from_day\": \"day\"}, inplace=True)\n",
    "df_in.rename(columns={\"to_station_id\": \"station_id\", \"to_day\": \"day\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And re-set the index to be the (station id, hour) pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_out.set_index([\"station_id\", \"day\"])\n",
    "df_in = df_in.set_index([\"station_id\", \"day\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join these two DataFrames to compute an `flow = out - in` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df_in.join(df_out, how=\"outer\").fillna(0).reset_index()\n",
    "full_df[\"flow\"] = full_df[\"out\"] - full_df[\"in\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"time\"] = (full_df[\"day\"] * one_day).astype('datetime64[ns]')\n",
    "full_df = full_df[[\"station_id\", \"time\", \"flow\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = []\n",
    "for i in stations.station_id:\n",
    "    subdf = full_df[full_df.station_id==i].drop(\"station_id\").set_index(\"time\")\n",
    "    flows.append((i, subdf.flow.max(), subdf.flow.min()))\n",
    "flows = pd.DataFrame(flows, columns=[\"station_id\", \"max_out\", \"max_in\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.iloc[flows.max_out.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.iloc[flows.max_in.argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows[flows.station_id.isin(weekday)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows[flows.station_id.isin(weekday)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = []\n",
    "\n",
    "for i in stations.station_id:\n",
    "    s = full_df[full_df.station_id==i][[\"time\", \"flow\"]]\n",
    "    s.rename(columns={\"flow\": f\"s{i}\"}, inplace=True)\n",
    "    s = s.set_index(\"time\")\n",
    "    series.append(s)\n",
    "    \n",
    "df_wide = cudf.concat(series, axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.hvplot(y=[\"s81\", \"s287\"], alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.hvplot(y=[f\"s{n}\" for n in weekday], alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_wide.hvplot(y=[f\"s{n}\" for n in weekend], alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(flows[flows.max_out > 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flows[flows.max_in < -20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cumulative = df_wide.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_shade(df):\n",
    "    cols = list(df.columns)\n",
    "    \n",
    "    itime = cudf.to_datetime(df.index).astype('int64')\n",
    "    x_range = (itime[0], itime[-1])\n",
    "    \n",
    "    y_range = (df.min().min(), df.max().max())\n",
    "    \n",
    "    temp = cudf.DataFrame(df)\n",
    "    temp[\"itime\"] = itime\n",
    "    \n",
    "    cvs = ds.Canvas(plot_height=400, plot_width=1000)\n",
    "    agg = cvs.line(temp, x=\"itime\", y=cols, agg=ds.count(), axis=1)\n",
    "    \n",
    "    return tf.shade(agg, how='eq_hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_shade(df_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "series_shade(df_cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"to_station_id\"]==268]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
