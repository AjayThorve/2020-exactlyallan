{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis with Visual Analytics\n",
    "\n",
    "**Combining some basic analytics with visualization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Requirements\n",
    "\n",
    "In thi notebook we will continue to explore the Divvy bikes dataset using a few new tools like [cugraph](https://docs.rapids.ai/api/cugraph/stable/), [cuspatial](https://github.com/rapidsai/cuspatial), and [cudf](https://docs.rapids.ai/api/cudf/stable/) and see how these results can easily feed direcly into visualization tools like [hvplot](https://hvplot.holoviz.org/user_guide/index.html) and [datashader](https://datashader.org/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "In addition to the libraries mentioned above, we will also make use of libraries [cupy](https://docs.cupy.dev/en/stable/), NumPy, and Pandas directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "import cupy\n",
    "import cuspatial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "\n",
    "import hvplot.cudf\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Data / Check\n",
    "\n",
    "First let's load the data. In addition to the main Divvy `data.csv` file, we will also load the small `stations.csv` file that we prepared in the first notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_csv(DATA_DIR / \"data.csv\", parse_dates=('starttime', 'stoptime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv(DATA_DIR / \"stations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will eventually want to also look for any patterns in weekday vs weekend. so let's first add a column for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"weekday\"] = df['starttime'].dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some analysis -> ? = profit?\n",
    "\n",
    "Mix of cuspatial / cugraph analysis to then visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to cuGraph / cuSpatial / cupy\n",
    "\n",
    "We will analyze the data using some RAPIDS tools:\n",
    "\n",
    "* [cuSpatial](https://docs.rapids.ai/api/cuspatial/nightly/) is a collection of GPU accelerated algorithms for computing geo-spatial measures. We can use to to compute trips with given bounding regions.\n",
    "\n",
    "* [cuGraph](https://docs.rapids.ai/api/cugraph/stable/) is a collection of GPU accelerated graph algorithms that process data found in GPU DataFrames. We can use to compute pagerank or degree measure on the trip graph. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuSpatial\n",
    "\n",
    "Let's take a look at some spatial measures and see if there are any interesting features.\n",
    "\n",
    "We might start with the first station, and see what the max trip length from it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0 = df.iloc[0]\n",
    "station_id, origin_lon, origin_lat = r0[\"from_station_id\"], r0[\"longitude_start\"], r0[\"latitude_start\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cuSpatial function `lonlat_to_cartesian` will let us quicly compute the x/y distances for every ending trip location. (Note that distances will in *kilometers*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df[df[\"from_station_id\"]==station_id[0]]\n",
    "dist = cuspatial.lonlat_to_cartesian(origin_lon[0], origin_lat[0], sub_df[\"longitude_end\"], sub_df[\"latitude_end\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CuPy functions can compute derived values on these GPU dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy.sqrt(cupy.max(dist.x**2 + dist.y**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to compute this all trip distances? We can compute the distances using every station as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trip_dists(df):\n",
    "    results = []\n",
    "\n",
    "    for idx, row in stations.iterrows():\n",
    "        station_id, origin_lon, origin_lat = int(row[\"station_id\"]), row[\"lon\"], row[\"lat\"]\n",
    "        sub_df = df[df[\"from_station_id\"]==station_id]\n",
    "        res = cuspatial.lonlat_to_cartesian(origin_lon, origin_lat, sub_df[\"longitude_end\"], sub_df[\"latitude_end\"])\n",
    "        res[\"dist\"] = cupy.sqrt(res.x**2 + res.y**2)\n",
    "        results.append(res)\n",
    "        \n",
    "    return cudf.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_from_dists = trip_dists(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_from_dists.hvplot.hist(y=\"dist\", normed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be instesting to break the distribution of trips down weekday vs weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend_trips = df[df[\"weekday\"].isin([5, 6])] # weekend days = 5, 6 \n",
    "weekday_trips = df[df[\"weekday\"].isin(list(range(5)))]  # weekday days = 0..4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend_dists = trip_dists(weekend_trips)\n",
    "weekday_dists = trip_dists(weekday_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combined_dists =  cudf.concat([weekday_dists, weekend_dists])\n",
    "all_combined_dists.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting these two distributions together we can see the weekday (orange) trips peak more at shorter distances and the weekend distributions has more longer trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weekend_hist = weekend_dists.hvplot.hist(y=\"dist\", alpha=0.3, bin_range=(0, 20), normed=True, color=\"blue\")\n",
    "weekday_hist = weekday_dists.hvplot.hist(y=\"dist\", alpha=0.3, bin_range=(0, 20), normed=True, color=\"orange\")\n",
    "weekend_hist * weekday_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuDF\n",
    "\n",
    "Let's use CuDF direclty to group and aggregate our data to look for anyting intersting about the flow of trips in and out stations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at the daily net flow of trips at each station, i.e. how many more (or less) trips *started* at a station vs *ended* at a station in a given day.\n",
    "\n",
    "In order to group by day, we first take the \"floor\" of each timestamp divided by one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = np.datetime64(1, 'D').astype('datetime64[ns]').astype('int64') \n",
    "\n",
    "df['from_day'] = df['starttime'].astype('int64') // one_day\n",
    "df['to_day'] = df['stoptime'].astype('int64') // one_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can group by the station id and hour for both the departing and arriving cases. We name the columns from the size DataFrame `out` and `in` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df.groupby(by=[\"from_station_id\", \"from_day\"]).size().to_frame('out').reset_index()\n",
    "df_in = df.groupby(by=[\"to_station_id\", \"to_day\"]).size().to_frame('in').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re name the columns to be the same in both DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.rename(columns={\"from_station_id\": \"station_id\", \"from_day\": \"day\"}, inplace=True)\n",
    "df_in.rename(columns={\"to_station_id\": \"station_id\", \"to_day\": \"day\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And re-set the index to be the (station id, hour) pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_out.set_index([\"station_id\", \"day\"])\n",
    "df_in = df_in.set_index([\"station_id\", \"day\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join these two DataFrames to compute an `flow = out - in` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df_in.join(df_out, how=\"outer\").fillna(0).reset_index()\n",
    "full_df[\"flow\"] = full_df[\"out\"] - full_df[\"in\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also convert our \"day\" values back to proper timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"time\"] = (full_df[\"day\"] * one_day).astype('datetime64[ns]')\n",
    "full_df = full_df[[\"station_id\", \"time\", \"flow\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a glimpse at the resulting DataFrame which has the net trip flow by station per day. A positive number means there was an excess of trips *starting* at station that day. A negative number indicates an excess of trips *ending* at a station that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might like to look at the maximal behaviour. What is a high number of excess arrivals or departures at a station? Let's pull out individual timeseries for each station id, and look a the max/min for each station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = []\n",
    "for i in stations.station_id:\n",
    "    subdf = full_df[full_df.station_id==i].drop(\"station_id\").set_index(\"time\")\n",
    "    flows.append((i, subdf.flow.max(), subdf.flow.min()))\n",
    "flows = pd.DataFrame(flows, columns=[\"station_id\", \"max_out\", \"max_in\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information, we can see what stations had the larges ever excess departures (station 192) or arrivals (station 77):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.iloc[flows.max_out.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.iloc[flows.max_in.argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing about execess arrivals vs departures is probably import for Divvy to be able to manually re-allocate bikes. We could ask what fraction of stations ever have a max of more than 30 excess trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(flows[flows.max_out > 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flows[flows.max_in < -30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would try to look at all of these trip flow series for each station at once, using Datashader. First we need to prepare a new Dataframe that has all the series as columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = []\n",
    "\n",
    "for i in stations.station_id:\n",
    "    s = full_df[full_df.station_id==i][[\"time\", \"flow\"]]\n",
    "    s.rename(columns={\"flow\": f\"s{i}\"}, inplace=True)\n",
    "    s = s.set_index(\"time\")\n",
    "    series.append(s)\n",
    "    \n",
    "df_wide = cudf.concat(series, axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting Dataframe has a time series for every column, one for each station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's simple to pull out individual stations for comparison using `hvplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.hvplot(y=[\"s81\", \"s287\"], alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets take a look at the data with datashader. First we make a funtion `series_shade` that can take a wide dataframe of timeseries like we make above, and render *all* of the series at once using datashaher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_shade(df):\n",
    "    cols = list(df.columns)\n",
    "    \n",
    "    itime = cudf.to_datetime(df.index).astype('int64')\n",
    "    x_range = (itime[0], itime[-1])\n",
    "    \n",
    "    y_range = (df.min().min(), df.max().max())\n",
    "    \n",
    "    temp = cudf.DataFrame(df)\n",
    "    temp[\"itime\"] = itime\n",
    "    \n",
    "    cvs = ds.Canvas(plot_height=400, plot_width=1000)\n",
    "    agg = cvs.line(temp, x=\"itime\", y=cols, agg=ds.count(), axis=1)\n",
    "    \n",
    "    print(f\"y range: ({y_range[0]}, {y_range[1]})\")\n",
    "    return tf.shade(agg, how='eq_hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pass in out daily net excess data to get a rough datashder plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_shade(df_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not completely clear what we can see here but it points to some ideas for future exporation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last experiment, let's make the same plot, but with *cumulative* excess trips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cumulative = df_wide.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "series_shade(df_cumulative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This a bit more interesting and points to the notion that Divvy must be engaging in a lot of continual re-allocation of its bikes to offset these excess trips. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagerank\n",
    "\n",
    "Now we will use the `cugraph.pagerank` function to see if there are patterns for the \"most popular\" stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single hour \n",
    "\n",
    "First, let's see what it looks like to compute page range for a single hour of the day, e.g. 5PM. First subset the data to only look at data for trips starting at that hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17 = df[df[\"hour\"]==17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next group by (from_station_id, to_station_id) and then take the group size to get all the unique indivual routes between stations that hour, and also the number of trips that took each of those routes:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g17 = df.groupby(by=[\"from_station_id\", \"to_station_id\"])\n",
    "routes17 = g17.size().reset_index()\n",
    "routes17.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a `cugraph.Graph` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = cugraph.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.from_cudf_edgelist(d17, source='from_station_id', destination='to_station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17_page = cugraph.pagerank(G)\n",
    "d17_page.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed pagerank on the network of trips, let's see which stations rank as most important at 5PM (on any day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17_top = d17_page.nlargest(10, \"pagerank\").to_pandas()\n",
    "d17_top.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting these stations we can see that at 5PM the most important stations are all downtown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d17_page_locs = stations[stations.station_id.isin(d17_top.vertex)]\n",
    "d17_page_locs.hvplot.points(x='lon', y='lat', size=300, geo=True, tiles=\"OSM\").opts(width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how stations rank week on weekdays vs weekends. The code below computes the pagerank broken out by individual day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for w in range(7):\n",
    "    dfw = df[df[\"weekday\"]==w]\n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(dfw, source='from_station_id', destination='to_station_id')\n",
    "    df_page = cugraph.pagerank(G).nlargest(20, \"pagerank\")\n",
    "    results[w] = set(df_page.to_pandas()[\"vertex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find out what stations were highest ranked among all weekdays and weekend days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = set.intersection(*[results[i] for i in range(5)]) # days 1..5 are weekdays\n",
    "weekend = set.intersection(results[5], results[6])  # days 5 and 6 are the weekend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the stations that are all import on weekdays, and all important on weekends (and that there is not much overlap):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can plot these quickly using `hvplot`. Let's add a column to denote weekday/weekend so that we can group by that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = stations[stations.station_id.isin(weekend)]\n",
    "r1 = r1.assign(type=\"Weekend\")\n",
    "\n",
    "r2 = stations[stations.station_id.isin(weekday)]\n",
    "r2 = r2.assign(type=\"Weekday\")\n",
    "\n",
    "result = pd.concat([r1, r2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot, nearly all the important weekday stations are downtown, and on the weekend the important stations are furhter out, in popular districts around downtown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.hvplot.points(x='lon', y='lat', by='type', \n",
    "                     alpha=0.5, size=485, geo=True, tiles=\"OSM\").opts(width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that the important weekday stations (for all hours) are similarly clustered downtown as the top rush-hour stations at 5pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What looking at our previous trip flow data with respect to weekday vs weekend? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows[flows.station_id.isin(weekday)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows[flows.station_id.isin(weekday)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g. we can look at all the weekday stations where we can see some stations are consistent in having more excess departures or arrivals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide.hvplot(y=[f\"s{n}\" for n in weekday], alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the weekend stations. Here there is one station with a clear excess of arrivals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_wide.hvplot(y=[f\"s{n}\" for n in weekend], alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hovering over the plot we can see that it is station 268 which turns out to be right by the waterfont museum district:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df[\"to_station_id\"]==268].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of interesting analytics results \n",
    "Does not have to be significant but noteable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
